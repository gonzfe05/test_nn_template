{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook hypotheses\n",
    "#### 1. Contrastative learning embedding has significantly better separability than PCA or tsne even when overfitted\n",
    "\n",
    "### Motivation\n",
    "Compare contrastative learning model against benchmarks that should perform worse.\n",
    "* PCA is a simple and fast linear method, shouldnt beat a contrastative model non-linear mapping in a complex problems.\n",
    "* t-sne does not learn a mapping from feature-space so cant be used as an embedding. If we cant beat t-sne then there is a better non-linear mapping we have not achived. \n",
    "\n",
    "### Actions:\n",
    "* Hypotheses N. 1 is `True`.\n",
    "    * Make a dataset of embeddings useful for training a classifier\n",
    "* Hypotheses N. 1 is `False`.\n",
    "    * Review contrastative learning model.\n",
    "\n",
    "### Results:\n",
    "* 1. `True`, contrastative embedding has orders of magnitude more separability. [Link](#techniques-separability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb metadata\n",
    "NB_NAME = \"dimentionality reduction.ipynb\"\n",
    "NB_PATH = \"notebooks\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastative embeddings dimentionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env WANDB_API_KEY=4f8699d18b665419da19c00aeb7291bcafb88ac5\n",
    "# import os\n",
    "# os.environ['WANDB_API_KEY'] = '4f8699d18b665419da19c00aeb7291bcafb88ac5'\n",
    "from omegaconf import DictConfig\n",
    "from test_nn_template.run import WandbHandler\n",
    "\n",
    "print(f\"Runs: {len(WandbHandler.get_runs(entity='fernandoezequiel512', project='test_nn_template'))}\")\n",
    "run_id = \"fernandoezequiel512/test_nn_template/runs/28vaq6y9\"\n",
    "run = WandbHandler.get_run(run_id)\n",
    "print(f\"Got {run.name}\")\n",
    "model = WandbHandler.load_run_model_checkpoint(run_id)\n",
    "print(model)\n",
    "\n",
    "\n",
    "def replace_dataset(cfg: DictConfig):\n",
    "    _target_train = cfg.data.datamodule.datasets.train._target_.replace(\"MyContrastativeDataset\", \"MyDataset\")\n",
    "    _target_test = cfg.data.datamodule.datasets.test._target_.replace(\"MyContrastativeDataset\", \"MyDataset\")\n",
    "    cfg.data.datamodule.datasets.train._target_ = _target_train\n",
    "    cfg.data.datamodule.datasets.test._target_ = _target_test\n",
    "    return cfg\n",
    "\n",
    "\n",
    "datamodule = WandbHandler.load_run_datamodule(run_id, cfg_func=replace_dataset)\n",
    "# print(datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup(stage=\"test\")\n",
    "test_dataloader = datamodule.test_dataloader()[0]\n",
    "train_dataloader = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from test_nn_template.data.datamodule import MyDataModule\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "x_contrast = []\n",
    "labels = []\n",
    "for x, y in test_dataloader:\n",
    "    # print(x.shape, y.detach().numpy())\n",
    "    emb = model.model.forward_once(x).detach().numpy()\n",
    "    x_contrast.append(emb)\n",
    "    labels.append(y)\n",
    "    # break\n",
    "x_contrast = np.concatenate(x_contrast)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "ixs = random.choices(range(x_contrast.shape[0]), k=500)\n",
    "\n",
    "plt.scatter(x_contrast[ixs, 0], x_contrast[ixs, 1], c=labels[ixs])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA dimentionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X = []\n",
    "labels = []\n",
    "for x, y in test_dataloader:\n",
    "    X.append(x.detach().numpy())\n",
    "    labels.append(y)\n",
    "X = np.concatenate(X).squeeze()\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "print(X.shape)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "print(f\"explained variance: {pca.explained_variance_ratio_}\")\n",
    "print(f\"singular values: {pca.singular_values_}\")\n",
    "\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "plt.scatter(X_pca[ixs, 0], X_pca[ixs, 1], c=labels[ixs])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-sne dimentionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = []\n",
    "labels = []\n",
    "for x, y in test_dataloader:\n",
    "    X.append(x.detach().numpy())\n",
    "    labels.append(y)\n",
    "X = np.concatenate(X).squeeze()\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "# print(X.shape)\n",
    "\n",
    "X_tsne = TSNE(n_components=2, learning_rate=\"auto\", init=\"random\", perplexity=3).fit_transform(X)\n",
    "X_tsne.shape\n",
    "\n",
    "labels = np.concatenate(labels)\n",
    "plt.scatter(X_tsne[ixs, 0], X_tsne[ixs, 1], c=labels[ixs])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "vars = [\"contrast_1\", \"contrast_2\", \"pca_1\", \"pca_2\", \"tsne_1\", \"tsne_2\"]\n",
    "X = np.concatenate([x_contrast, X_pca, X_tsne], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, stratify=labels, random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "start_time = time.time()\n",
    "result = permutation_importance(forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "forest_importances = pd.Series(result.importances_mean, index=vars)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embeddings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Callable, Optional\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from wandb.apis.public import Run\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "source = os.path.join(NB_PATH, NB_NAME)\n",
    "transform = lambda x: model.model.forward_once(x)\n",
    "# for x, y in test_dataloader:\n",
    "#     emb = model.model.forward_once(x)\n",
    "\n",
    "\n",
    "class EmbeddingsSaver(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        run: Run,\n",
    "        train_dataloader: DataLoader,\n",
    "        test_dataloader: DataLoader,\n",
    "        source: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.train = train_dataloader\n",
    "        self.test = test_dataloader\n",
    "        self.run = run\n",
    "        self.run_id = run.name\n",
    "        self.metadata = self.build_metadata(source)\n",
    "\n",
    "    def build_metadata(self, source):\n",
    "        self.metadata = {\n",
    "            \"wdb_entity\": self.run.entity,\n",
    "            \"wdb_project\": self.run.project,\n",
    "            \"wdb_run_id\": self.run.name,\n",
    "            \"source\": source,\n",
    "        }\n",
    "\n",
    "    def save_embeddings(self, dataloader: DataLoader, paths: dict) -> torch.Tensor:\n",
    "        for ix, (x, y) in tqdm(enumerate(dataloader), desc=\"Embedding\"):\n",
    "            path = os.path.join(paths[\"embeddings\"], f\"{ix}.pt\")\n",
    "            self.save_tensor(transform(x), path)\n",
    "            path = os.path.join(paths[\"labels\"], f\"{ix}.pt\")\n",
    "            labels.append(y)\n",
    "        embeddings = torch.cat(embeddings)\n",
    "        labels = torch.cat(y)\n",
    "        return embeddings, labels\n",
    "\n",
    "    def save_tensor(self, tensor: torch.Tensor, path: str) -> None:\n",
    "        torch.save(tensor, path)\n",
    "\n",
    "    def save_dict(self, dict: dict, path: str) -> None:\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dumps(dict, f)\n",
    "\n",
    "    def save(self, root: str) -> None:\n",
    "        struct = self.build_folders_struct(root, self.run_id)\n",
    "        self.save_dict(self.metadata, struct[\"metadata\"])\n",
    "        train_emb, train_lab = self.save_embeddings(self.train, struct[\"train\"])\n",
    "        test_emb, test_lab = self.save_embeddings(self.test, struct[\"test\"])\n",
    "\n",
    "    @classmethod\n",
    "    def build_folders_struct(cls, root: str, run_id: str) -> dict:\n",
    "        def join(args):\n",
    "            return os.path.join(*args)\n",
    "\n",
    "        def get_base_struct(split):\n",
    "            return {\"embeddings\": join([root, split, \"embeddings\"]), \"labels\": join([root, split, \"labels\"])}\n",
    "\n",
    "        root = join([root, run_id])\n",
    "        struct = {\n",
    "            \"metadata\": join([root, \"metadata\", \"metadata.json\"]),\n",
    "            \"train\": get_base_struct(\"train\"),\n",
    "            \"test\": get_base_struct(\"test\"),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(struct[\"metadata\"]))\n",
    "        os.makedirs(os.path.dirname(struct[\"train\"][\"embeddings\"]))\n",
    "        os.makedirs(os.path.dirname(struct[\"test\"][\"embeddings\"]))\n",
    "        return struct\n",
    "\n",
    "\n",
    "embeddings_saver = EmbeddingsSaver(run, train_dataloader, test_dataloader, source)\n",
    "embeddings_saver.save(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(torch.cat(embeddings), 'test.pt')\n",
    "os.path.dirname(\"a/b/c.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_nn_template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "15811d5068274a7cadd482672f677129ba5c55e94d38dd440bd9a4cbce4a0b06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
